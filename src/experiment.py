"""
Runs all the explainers to generate explanations.  This file is derived from
all_explainers.py and runs the experimental pipeline

Date :                  September 13th, 2019

Updates:
    2019-11-05                          Add outputs of true/false positives/negatives

"""

use_tf = True

if use_tf:
    import tensorflow.keras as keras
else:
    import keras
from keras.models import load_model
from keras.utils import CustomObjectScope
from keras.initializers import glorot_uniform

import innvestigate
import innvestigate.utils as iutils

from lime_explainer import explain_lime
from innvestigate_explainer import explain_innvestigate
from protodash_explainer import explain_protodash
from explanation_images import create_explanation_images
from cnn import CNN
from image_dataloader import Dataloader
from utils import plot_numpy_images, print_iterables_file

from skimage.color import gray2rgb

from datetime import datetime
import numpy as np

import os

def populate_text_dict(text_file, text_dict):
    """
    Inputs explanation text in a .txt file into a dictionary.  The .txt file is formatted
    such that the filename is the explainer name, and every text (multiple lines) for every
    explanation is separated by an empty new line (i.e., "\n").  The 
    """
    
    #   get name of text file
    explainer_name = os.path.basename(text_file)
    explainer_name = os.path.splitext(explainer_name)[0]
    
    #   if text file name in text_dict, raise ValueError
    if explainer_name in text_dict:
        raise ValueError("Explainer name already in text_dict.")
    else:
        text_dict[explainer_name] = []
    
    with open(text_file, "r") as read_file:
        text = read_file.readlines()                #   list with each line as a file
        
    #   for every "section" in .txt, add each line into list
    section = []
    for line in text:
        if line == "\n":            #   if empty new line
            text_dict[explainer_name].append(section)
            section = []
        else:
            #   remove next line character
            if line[-1] == "\n":
                line = line[:-1]
            section.append(line)
    text_dict[explainer_name].append(section)        #   append final section
    
    return text_dict


def combine_single_explain_images(image_files, orig_files, num_images,
                                  explanation_name, image_names, explain_params, save_dir=""):
    """
    Combine explanation images generated by explainers into single .png files.  The images combined follows format
    of having orig_files in one row, followed with image_files in the next row.
    Used for LIME and heatmaps.
    
    Arguments:
        image_files (lst[str,])     :           list of explanation filenames to combine
        orig_files (lst[str,]) :                list of original filenames corresponding to image_files
        num_images (int) :                      number of images per combination
        explanation_name (str) :                name of explanation (found in text_dict)
        image_names (lst[str, ]) :              filenames of combined images
        explain_params (dict{}) :               parameters for explanation (see create_explanation_images) 
        save_dir (str) :                        directory to save combined images to
    """
    background_size = (1400, 800)          #   preset with trial and error
    text_dict = explain_params['text_dict']
    
    if explanation_name not in text_dict:
        raise ValueError("Can't find explanation_name ", explanation_name, " in text_dict.")
    else:
        explanation_text_lst = text_dict[explanation_name]
        #   this is a list of sublists, where each sublist contains text for each set of images to combine
    
    idx = 0
    
    #   for every set of images to combine (e.g. top_indices_0, etc.)
    for i in range(0, len(image_files), num_images):
        text = explanation_text_lst[idx]
        explanation_images = image_files[i:i+num_images]            #   explanation images
        original_images = orig_files[i:i+num_images]
        """
        print("original", original_images)
        print("explant", explanation_images)
        """
        image_series_lst = [original_images, explanation_images]
        create_explanation_images(text, image_series_lst, explain_params, save_dir=save_dir, 
                                  save_name=explanation_name+"_"+image_names[idx]+".png", 
                                  background_size=background_size)
        idx += 1
        
def combine_multi_explain_images(image_files, orig_files, num_images,
                                explanation_name, image_names, explain_params, save_dir=""):
    """
    Combine explanation images generated by explainers into single .png files.  The images combined follows
    format of having 1 orig_file and 1 image_file per row.
    
    Arguments:
        image_files (lst[str,])     :           list of explanation filenames to combine
        orig_files (lst[str,]) :                list of original filenames corresponding to image_files
        num_images (int) :                      number of images per combination
        explanation_name (str) :                name of explanation (found in text_dict)
        image_names (lst[str, ]) :              filenames of combined images
        explain_params (dict{}) :               parameters for explanation (see create_explanation_images) 
        save_dir (str) :                        directory to save combined images to
    """
    background_size = (1000, 1100)

    text_dict = explain_params['text_dict']
    
    if explanation_name not in text_dict:
        raise ValueError("Can't find explanation_name ", explanation_name, " in text_dict.")
    else:
        explanation_text = text_dict[explanation_name]
        #   this is a list of sublists, where each sublist contains text for each set of images to combine
    
    if len(image_files) != len(orig_files):
        raise Exception("Uneven lengths for image_files and orig_files.")
    
    idx = 0

    print("Explanation_text: ", explanation_text)

    #   for every set of images to combine (e.g., top_indices_0, etc.) 
    for i in range(0, len(image_files), num_images):
        image_series_lst = []
        if idx < len(explanation_text):
            text = explanation_text[idx]
        else:
            text = ""   #   no text
        
        #   for every original image, create a new row 
        for image_idx in range(i, i+len(image_files[i:i+num_images])):
            image_series_lst.append([orig_files[image_idx], image_files[image_idx]])    #   each sublist in here is 1 row
        
        create_explanation_images(text, image_series_lst, explain_params, save_dir=save_dir, 
                                  save_name=explanation_name+"_"+image_names[idx]+".png",
                                  background_size=background_size, charwidth=70)
        idx += 1
                
        
def order_indices_by_accuracy(y_test, y_pred, data=None):
    """
    Order indices and their corresponding accuracies in terms of most accurate to least accurate.
    Arguments:
        y_test (np.array) :         binary ground truth labels (num_samples,)
        y_pred (np.array) :         predicted labels
        data (list[]) :             list of data to be ordered (optional)
    Returns:
        2 lists of tuples, each tuple containing the following:
            data (optional), label, prediction, inaccuracy, original index
    """
    idx_0, diff_0, data_0, y_pred_0, y_test_0 = [], [], [], [], []
    idx_1, diff_1, data_1, y_pred_1, y_test_1 = [], [], [], [], []

    #   for every sample
    for idx in range(0, y_test.shape[0]):       
        if y_test[idx] == 0:
            idx_0.append(idx)
            diff_0.append(y_pred[idx] - y_test[idx])        #   lower the diff the better
            y_pred_0.append(y_pred[idx])
            y_test_0.append(y_test[idx])
            if data != None:
                data_0.append(data[idx])
        else:
            idx_1.append(idx)
            diff_1.append(y_test[idx] - y_pred[idx])        
            y_pred_1.append(y_pred[idx])
            y_test_1.append(y_test[idx])
            if data != None:
                data_1.append(data[idx])
    
    if data != None:
        #   sort (smallest first) based on diff
        tuples_0 = [(data, y_test, y_pred, diff, idx) for diff, idx, data, y_pred, y_test 
                    in sorted(zip(diff_0, idx_0, data_0, y_pred_0, y_test_0))]
        tuples_1 = [(data, y_test, y_pred, diff, idx) for diff, idx, data, y_pred, y_test 
                    in sorted(zip(diff_1, idx_1, data_1, y_pred_1, y_test_1))]
    else:
        #   sort (smallest first) based on diff
        tuples_0 = [(y_test, y_pred, diff, idx) for diff, idx, y_pred, y_test 
                    in sorted(zip(diff_0, idx_0, y_pred_0, y_test_0))]
        tuples_1 = [(y_test, y_pred, diff, idx) for diff, idx, y_pred, y_test 
                    in sorted(zip(diff_1, idx_1, y_pred_1, y_test_1))]

    return tuples_0, tuples_1

def separate_to_positives_negatives(data, labels, predicts, threshold=0.5):
    """
    Separate to true positives, true negatives, false positives, false negatives.
    Arguments:
        data (list[, ]) :               list of individual samples
        labels (list[int, ]) :          list of binary labels (0 or 1)
        predicts (list[float, ]) :      list of predictions (0.0 to 1.0)
        threshold (float) :             threshold for categorizing prediction as positive or negative
    Returns:
        4 lists of tuples(data, label, prediction) corresponding to 
            true positives, true negatives, false positives, false negatives
    """
    
    #   check if same length
    if len(data) != len(labels) or len(labels) != len(predicts):
        raise ValueError("Different input lengths.")
    
    tp = []
    tn = []
    fp = []
    fn = []
    
    #   for each sample, add to appropriate category
    for idx in range(0, len(data)):
        
        #   real negative
        if labels[idx] == 0:
            if predicts[idx] >= threshold:
                fp.append((data[idx], labels[idx], predicts[idx]))
            else:
                tn.append((data[idx], labels[idx], predicts[idx]))
        #   real positive
        else:
            if predicts[idx] >= threshold:
                tp.append((data[idx], labels[idx], predicts[idx]))
            else:
                fn.append((data[idx], labels[idx], predicts[idx]))
        
    return tp, tn, fp, fn

def experiment(dl_params, model_params, explain_params, save_dir):
    """
    Experimental pipeline.
    """
    
    #   load dataset
    print("Loading dataset...")
    dataloader = Dataloader(dl_params, rseed=0)
    #   outputs are not in batches, but list of all samples
    X_train, y_train = dataloader.get_dataset_images_and_labels("train")
    X_valid, y_valid = dataloader.get_dataset_images_and_labels("valid")
    X_test, y_test = dataloader.get_dataset_images_and_labels("test")
    
    X_test_filenames, y_test_labels = dataloader.get_dataset_names_and_labels("test")
        
    del dataloader      #   save some memory
    
    #   convert to np.array
    X_train = np.stack(X_train, axis=0)
    X_valid = np.stack(X_valid, axis=0)
    X_test = np.stack(X_test, axis=0)
    y_train = np.asarray(y_train)
    y_valid = np.asarray(y_valid)
    y_test = np.asarray(y_test)

    #   normalize to between 0 and 1
    X_train = X_train.astype("float") / 255.0
    X_valid = X_valid.astype("float") / 255.0
    X_test = X_test.astype("float") / 255.0
    
    print("X_train.shape: ", X_train.shape)
    
    #   convert from grayscale to rgb image(LIME requires this...annoying af)
    if X_train.shape[-1] == 1:                       #   if grayscale
        print("Converting from grayscale to RGB...")
        X_train = gray2rgb(X_train.squeeze(axis=-1))
        X_valid = gray2rgb(X_valid.squeeze(axis=-1))
        X_test = gray2rgb(X_test.squeeze(axis=-1))
    
    
    #   getting prototypes for each class in entire training datasets
    folder = save_dir + "class_0/"
    if os.path.exists(folder) is False:
        os.makedirs(folder)
    proto_indices, weights, _ = explain_protodash((X_train, y_train), (X_train, y_train), 
                                                  label=0, num_protos=5, 
                                                  filename="proto",
                                                  save_dir=folder)
    folder = save_dir + "class_1/"
    if os.path.exists(folder) is False:
        os.makedirs(folder)
    proto_indices, weights, _ = explain_protodash((X_train, y_train), (X_train, y_train), 
                                                  label=1, num_protos=5, 
                                                  filename="proto",
                                                  save_dir=folder)
    
    #   load model into classifier
    print("Loading pre-existing classifier...")
    #   add this line to prevent some Keras serializer error
    with CustomObjectScope({'GlorotUniform': glorot_uniform()}):
        model = load_model(model_params['load_location'])
    
    #   run prediction
    model.summary()
    y_pred = model.predict(X_test)
    
    print("Ypred length: ", len(y_pred))
    print("X_test: ", len(X_test))
    
    #   sort into true positives, true negatives, false positives, false negatives
    tp, tn, fp, fn = separate_to_positives_negatives(X_test_filenames, y_test, y_pred)
    
    print("TP: ", len(tp))
    print("TN: ", len(tn))
    print("FP: ", len(fp))
    print("FN: ", len(fn))
    
    #   create files listing prediction and label for each accuracy category
    data, labels, preds = zip(*tp)
    _, ordered_tp = order_indices_by_accuracy(np.asarray(labels), np.asarray(preds), data)
    data, labels, preds, errors, idx = zip(*ordered_tp)
    print_iterables_file([data, labels, preds, errors, idx], save_name="tp", save_dir=save_dir)
    data, labels, preds = zip(*tn)
    ordered_tn, _ = order_indices_by_accuracy(np.asarray(labels), np.asarray(preds), data)
    data, labels, preds, errors, idx = zip(*ordered_tn)
    print_iterables_file([data, labels, preds, errors, idx], save_name="tn", save_dir=save_dir)
    data, labels, preds = zip(*fp)
    ordered_fp, _ = order_indices_by_accuracy(np.asarray(labels), np.asarray(preds), data)
    data, labels, preds, errors, idx = zip(*ordered_fp)
    print_iterables_file([data, labels, preds, errors, idx], save_name="fp", save_dir=save_dir)
    data, labels, preds = zip(*fn)
    _, ordered_fn = order_indices_by_accuracy(np.asarray(labels), np.asarray(preds), data)
    data, labels, preds, errors, idx = zip(*ordered_fn)
    print_iterables_file([data, labels, preds, errors, idx], save_name="fn", save_dir=save_dir)
    
    print("orderd tp: ", len(ordered_tp))
    print("orderd tn: ", len(ordered_tn))
    print("orderd fp: ", len(ordered_fp))
    print("orderd fn: ", len(ordered_fn))
    
    #   get most accurate and least accurate for each class
    ordered_indices_0, ordered_indices_1 = order_indices_by_accuracy(y_test, y_pred)
    
    #   get top indices
    top_indices_0 = [idx for _, _, _, idx in ordered_indices_0]
    top_indices_1 = [idx for _, _, _, idx in ordered_indices_1]
    top_diff_0 = [diff.item() for _, _, diff, _ in ordered_indices_0]
    top_diff_1 = [diff.item() for _, _, diff, _ in ordered_indices_1]
    
    num_indices = explain_params['num_images']
    image_names = ['top_indices_0', 'top_indices_1', 
                   'worst_indices_0', 'worst_indices_1']        #   name of output files
    
    top_indices = top_indices_0[:num_indices] + top_indices_1[:num_indices]
    worst_indices = top_indices_0[-1*num_indices:] + top_indices_1[-1*num_indices:]
    
    samples_to_explain = top_indices + worst_indices
    print(samples_to_explain)

    #   create folder system
    foldernames = [save_dir+'sample_'+str(idx)+"_class_"+str(y_test[idx])+"_pred_" + str(round(y_pred[idx].item(), 3)) + "/" 
                   for idx in samples_to_explain]
    for folder in foldernames:
        if os.path.exists(folder) is False:
            os.makedirs(folder)
    print(foldernames)
    
    #   plot original images
    original_image_files = []
    for i in range(0, len(samples_to_explain)):
        idx = samples_to_explain[i]
        folder = foldernames[i]
        image = np.array([X_test[idx, :, :, :]])
        label = np.array([y_test[idx]])
        filename = "original_"+str(idx)
        plot_numpy_images((image, label), filename=filename, save_dir=folder)
        original_image_files.append(folder + filename+"_"+str(label[0])+".png")    
   
    
    #   group images based on their classification values (e.g., top_indices_0, worst_indices_1)
    top_indices_folders_0 = foldernames[:num_indices]
    top_indices_folders_1 = foldernames[num_indices:2*num_indices]
    worst_indices_folders_0 = foldernames[2*num_indices:3*num_indices]
    worst_indices_folders_1 = foldernames[3*num_indices:]
    
    #   run LIME    
    num_superpixels = 10
    imagenet = explain_params['imagenet']
    lime_image_files = []
    print("Creating LIME explanations...")
    for i in range(0, len(samples_to_explain)):
        print("Sample: ", i)
        idx = samples_to_explain[i]
        folder = foldernames[i]
        #   get image, label corresponding to idx
        image = X_test[idx, :, :, :]
        label = y_test[idx]
        
        image_file = explain_lime(image, label, model, num_superpixels=num_superpixels, 
                                  save_name="lime_"+str(idx), save_dir=folder, imagenet=imagenet)
        lime_image_files.append(image_file)
    
    print("num_indices: ", num_indices)
    
    #   plot LIME explanations
    explanation_name = 'lime'
    combine_single_explain_images(lime_image_files, original_image_files, num_indices, explanation_name,
                                  image_names, explain_params, save_dir=save_dir)
    
    #   run heatmap
    print("Creating heatmap explanations...")
    #   select analyzer
    explainer_type = "deep_taylor"
    if model_params['output_dim'] > 2:
        model_wo_sm = iutils.keras.graph.model_wo_softmax(model)    #   remove softmax
    else:
        model_wo_sm = model
    analyzer = innvestigate.create_analyzer(explainer_type, model_wo_sm)
    
    heatmap_image_files = []
    for i in range(0, len(samples_to_explain)):
        idx = samples_to_explain[i]
        folder = foldernames[i]
        #   get image, label corresponding to idx
        image = X_test[idx, :, :, :]
        label = y_test[idx]
        image_file = explain_innvestigate(image, label, analyzer, 
                                          save_name="heatmap_"+str(idx), save_dir=folder)
        heatmap_image_files.append(image_file)
        
    #   plot heatmap explanations
    explanation_name = 'heatmap'
    combine_single_explain_images(heatmap_image_files, original_image_files, num_indices, explanation_name,
                                  image_names, explain_params, save_dir=save_dir)        
    
    print("Creating Protodash explanations...")
    proto_image_files = []
    for i in range(0, len(samples_to_explain)):
        print("Identifying prototypes for: ", idx)
        idx = samples_to_explain[i]
        folder = foldernames[i]
        image = np.array([X_test[idx, :, :, :]])        #   convert to shape (1 * height * width * num_layers)
        image_label = np.array([y_pred[idx]])           #   use the predicted label, not the ground truth label
        proto_indices, weights, image_file = explain_protodash((image, image_label), (X_train, y_train), 
                                               label=None, num_protos=3, mark_label=True, font_file=explain_params['font_file'],
                                               label_names= dl_params['labels'], filename="proto_"+str(idx),
                                               save_dir=folder)
        proto_image_files.append(image_file)
    
    #   plot prototype explanations
    explanation_name = "prototypes"
    
    print(proto_image_files)
    
    combine_multi_explain_images(proto_image_files, original_image_files, num_indices, explanation_name,
                                 image_names, explain_params, save_dir=save_dir)
    
    print("Experiment completed.")

if __name__ == "__main__":
    
    num_epochs = 300
    batch_size = 256
    
    dl_params = {
            'labels':           ['sun', 'moon'],
            'label_type':       'int',
            'presorted':        True,
            'file_locs':        ["../data/kuzushiji-49/0", "../data/kuzushiji-49/33"],
            'file_exten':       '.png',
            'set_ratio':        [0.8, 0.2],
            'batch_size':       batch_size,
            'target_size':      (28,28),
            'balanced':         True,
            'grayscale':        True,
            'superspeedmode':   False                #   trades off memory efficiency for less computation (USE AT YOUR OWN RISK)
    }
    
    
    model_params = {
        'output_dim':           1,
        'activation':           'relu',
        'load_location':        './model/model_e299.h5'
    }
    
    
    #   dictionary containing each explainer as key and list of list of text, with each
    #   list within a list contains text for each explanation with an explainer
    
    text_files = ['lime.txt', 'heatmap.txt', 'prototypes.txt']
    text_dict = {}
    for file in text_files:
        text_dict = populate_text_dict(file, text_dict)
    
    print(text_dict['prototypes'])
    
    explain_params = {
            'border' :          (0, 0),
            'num_images':       3,
            'font_file' :       'Arial.ttf',
            'text_size' :       30,
            'text_dict':        text_dict,
            'imagenet':         False
            }
    
    #   create save directory
    save_dir = "./output/"
     
    experiment(dl_params, model_params, explain_params, save_dir)